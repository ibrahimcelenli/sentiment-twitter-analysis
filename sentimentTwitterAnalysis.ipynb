{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids() #in the twitter_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#location => C:\\Users\\N550JV\\AppData\\Roaming\\nltk_data\\corpora\\twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(stopwords.words(\"english\")) #stopwords (ineffective word)\n",
    "\n",
    "#print(movie_reviews.fileids('neg'))\n",
    "#print(len(movie_reviews.fileids('neg')))\n",
    "\n",
    "#for fileid in movie_reviews.fileids('neg'):\n",
    "    #words = movie_reviews.words(fileid)   #Separation of words\n",
    "    #print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hopeless for tmr :(\n",
      "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "@Hegelbon That heart sliding into the waste basket. :(\n",
      "‚Äú@ketchBurning: I hate Japanese call him \"bani\" :( :(‚Äù\n",
      "\n",
      "Me too\n",
      "Dang starting next week I have \"work\" :(\n",
      "oh god, my babies' faces :( https://t.co/9fcwGvaki0\n",
      "@RileyMcDonough make me smile :((\n",
      "@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln\n",
      "why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"\n",
      "Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #‚Ä¶ http://t.co/dZZdqmf7Cz\n",
      "I have a really good m&amp;g idea but I'm never going to meet them :(((\n",
      "@Rampageinthebox mare ivan :(\n",
      "@SophiaMascardo happy trip, keep safe. see you soon :* :(\n",
      "I'm so tired hahahah :(\n",
      "@GrumpyCockney With knee replacements they get you up &amp; about the same day. :-(   Ouch.\n",
      "relate to the \"sweet n' sour\" kind of \"bi-polar\" people in your life... cuz my life... is FULL of them... :(\n",
      "@aysegul_k pleasse :(\n",
      "@SexyKalamo im not sure tho :(\n",
      "I feel stupid\n",
      "I just can't seem to grasp the basics of digital painting and nothing I've been researching is helping any :(\n",
      "Good Lord. :( https://t.co/nC9LkYUUvO\n",
      "I feel lonely someone talk to me guys and girls :(\n",
      "\n",
      "@TheOnlyRazzYT @imarieuda @EiroZPegasus @AMYSQUEE @UdotV\n",
      "No Assignment, but we have Project. :( really? üò©\n",
      "just want to play video games/watch movies with someone :(\n",
      "choreographing is hard : (\n",
      "@xo_raaaaayyy_xo what the email link? Still says that it's no longer available :( http://t.co/iuiaIOynnx\n",
      "cries bc i miss mingming so much :-(\n",
      "Sorry :( https://t.co/Q5TAYjrQ8K\n",
      "@Giannivnni mom so far away :(\n",
      "We're truly sorry @chrisbrown :( have a safe flight.\n",
      "and my friends :(\n",
      "@bbygjrlmgc oh :( i hate when that happens i get so sad over it too\n",
      "Oh. Dog has pee‚Äôd in my @Kneewax bag.  :-(   So I can‚Äôt take it to #NewWine15\n",
      "@YM_Dish98 doushite :( ?\n",
      "@Charliescoco @reeceftcharliie @SimonCowell too late :(\n",
      "It sucks so much been sick i was plan to start work on my first gundam to night but nope. :(\n",
      "MY $$$$2 DOLLAR :( üò≠üò≠üò≠üò≠üò≠üò≠ http://t.co/oI0pYGUsDi\n",
      "@martylog Listening back to old @DaveGorman shows (I know, I'm weird). Just got to u leaving: might give up. It was pale imitation after :-(\n",
      "i went in the sea and now have a massive fucking rash all over my body and it's the most painful thing ever i want to go home :((\n",
      "@dethronedlwt hi. Why are you absent? :(\n",
      "My Gran tho !!! She knew but didn't care to tell me :((\n",
      "@rowysoIjp SAME IT'S SO CUTE I LOVE IT SO MUCH I WISH THERE WOULD BE A SEQUEL :(\n",
      "@imallyssagail busy sa school :( next time love yah! xx\n",
      "Ouucchhh one of my wisdom teeth are coming through :(\n",
      "@StevenLDN frightening case. It really gets to you :(\n",
      "pret :(( wkwkw\"@WLK_Jhope: Verfied @WLK_Hyemi91 be active, don't forget to follow all member. Thanks for join. Goodbye\"\n",
      "You¬¥ve got me in chains for your love :¬¥( ‚Äî a sentir-se incompleta\n",
      "it's okay.. but.. :((\n",
      "@njhftbiebs why didn't you go on Wednesday :(\n",
      "@radicalj  Marvellous - not. How very thwarting :-(\n",
      "@ceeels95 Awh what's the chances üò© when u off to Zante? We need to do something :-( x\n",
      "@s0ulfl0wr When's your birthday ? :(\n",
      "@brittleyouth @Tom_J_Allen @AndrewFairbairn @batemanesque @Hegelbon @jameswheeler that was the worst part and I still feel bad about it :(\n",
      "audraesar: All these sushi pics on my tl are driving me craaaazzyy :(\n",
      "Really want this :( http://t.co/36tSy81iMi\n",
      "Popped like a helium balloon..  :-(\n",
      "#ClimateChange #CC California's powerful and influential air pollution watchdog.: Califor... http://t.co/OVU4p2qWfH #UniteBlue #Tcot :-(\n",
      "@itsNotMirna I was so sad because Elhaida was robbed by the juries :( she came 10th in the televoting\n",
      "#ClimateChange #CC Idaho will not restrict fishing despite regional drought-linked die-of... http://t.co/jJboDo6LYZ #UniteBlue #Tcot :-(\n",
      "#ClimateChange #CC Abrupt climate change may have doomed mammoths and other megafauna, sc... http://t.co/taVMCz37E7 #UniteBlue #Tcot :-(\n",
      "#ClimateChange #CC Australia's 'dirtiest' power station considers 'clean energy' biomass ... http://t.co/YeQABq6tsL #UniteBlue #Tcot :-(\n",
      "#ClimateChange #CC It ain't easy being green if you're a golf course in California.: Ulti... http://t.co/La82RXzTs2 #UniteBlue #Tcot :-(\n",
      "@Mess0019 Well I am sure your work day is over before mine :(\n",
      "@keikopotato IM GONNA MISS U SEXY PREXY :(((((((((\n",
      "i miss my kindergarten kids :(\n",
      "HUNGRY :-(\n",
      "cant find the only book that keeps me sane :((\n",
      "Literally there are three lounge events why :-( So much turn up I'm sad\n",
      "@carliot23 Miss you Boss :(\n",
      "I love hozier :-(\n",
      "@IzzyTailford that's true, I just want it soooooooner :(\n",
      "Ahh Fam @MeekMill :( #RespectLost http://t.co/NT25MYnGYd\n",
      "@shokako1104 I'm sorry :( ! What is hypercholesteloremia ? Are you ok ?\n",
      "Baby still looks tired :(\n",
      "Can someone gift me calibraska :(\n",
      "@Rico_Shabbir the massive shame about it is we would actually be genuine contenders but it won't happen :(\n",
      "My head always hurts if I stay up late lmao :(\n",
      "@ErnestLozoya how are u older than me :(\n",
      "Backed out :(\n",
      "u sound upset :( https://t.co/JZBFBKld8Q\n",
      "So much misses :-( https://t.co/pn2HvGdnFT\n",
      "I MISS INFINITE :-(\n",
      "@DiscoQing AoS doesn't do it for me but I don't want to stick with 8th either. :(\n",
      "@madrigalandreaa its too much  serious yun eh :(\n",
      "My room is way too hot :-(\n",
      "I still havent found my Handsome Jack drawing :((((\n",
      "@twcxmina shit :(\n",
      "But cut encore :(((( #bad4thwin\n",
      "I wish I had my own Baymax :(\n",
      "Sick :(\n",
      "@LittleMix French mixers miss you so much :( üíú\n",
      "Wft.. can't watch the awesome replay!! :-( https://t.co/ChzrqtelPh\n",
      "@Viiiiiiiiev what happened?? :(((\n",
      "Party promotions are over :(\n",
      "Music bank encore always so short :(\n",
      "BABY BOY :((((\n",
      "@flipkartsupport my order has been received at the hub nearest but doesn't look like it will be delivered today :( (1/2)\n",
      "Why is my mum playing music out loud :(\n",
      "The finale of Parasyte fucked my feelings alllllll the way up :(\n",
      "i wish :( #ZaynIsComingBackOnJuly26\n",
      "Good bye Party era :(\n"
     ]
    }
   ],
   "source": [
    "strings = twitter_samples.strings('negative_tweets.json')\n",
    "for string in strings[:100]:  #first 5 negative data.\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown': True, 'fox': True, 'quick': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Naive Bayes classification, you must label it explicitly.\n",
    "#I am removing the stopwords\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\") ]\n",
    "    \n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return my_dict\n",
    "\n",
    "create_word_features([\"the\", \"quick\", \"brown\", \"quick\", \"a\", \"fox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'@Hegelbon': True, 'That': True, 'heart': True, 'sliding': True, 'waste': True, 'basket': True, '.': True, ':(': True}, 'negative')\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()  \n",
    "\n",
    "neg_reviews = []\n",
    "for fileid in twitter_samples.strings('negative_tweets.json'):\n",
    "    words = tknzr.tokenize(fileid)\n",
    "    neg_reviews.append((create_word_features(words), \"negative\"))\n",
    "    \n",
    "print(neg_reviews[2])    \n",
    "print(len(neg_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'#FollowFriday': True, '@France_Inte': True, '@PKuchly57': True, '@Milipol_Paris': True, 'top': True, 'engaged': True, 'members': True, 'community': True, 'week': True, ':)': True}, 'positive')\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "pos_reviews = []\n",
    "\n",
    "for fileid in twitter_samples.strings('positive_tweets.json'):\n",
    "    words = tknzr.tokenize(fileid)\n",
    "    pos_reviews.append((create_word_features(words), \"positive\"))\n",
    "    \n",
    "print(pos_reviews[0])    \n",
    "print(len(pos_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 3000\n"
     ]
    }
   ],
   "source": [
    "train_set = neg_reviews[:3500] + pos_reviews[:3500]\n",
    "test_set =  neg_reviews[3500:] + pos_reviews[3500:]\n",
    "print(len(train_set),  len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "testing_set = test_set\n",
    "test_result = []\n",
    "gold_result = []\n",
    "\n",
    "for i in range(len(testing_set)):\n",
    "    test_result.append(classifier.classify(testing_set[i][0]))\n",
    "    gold_result.append(testing_set[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |    n    p |\n",
      "         |    e    o |\n",
      "         |    g    s |\n",
      "         |    a    i |\n",
      "         |    t    t |\n",
      "         |    i    i |\n",
      "         |    v    v |\n",
      "         |    e    e |\n",
      "---------+-----------+\n",
      "negative |<1490>  10 |\n",
      "positive |   14<1486>|\n",
      "---------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy:99.2\n",
      "\n",
      "label     \tprecision          \trecall    \tf_measure\n",
      "negative\t0.9906914893617021\t0.9933333333333333\t0.9920106524633822\n",
      "positive\t0.9933155080213903\t0.9906666666666667\t0.9919893190921228\n"
     ]
    }
   ],
   "source": [
    "CM = nltk.ConfusionMatrix(gold_result, test_result)\n",
    "print(CM)\n",
    "\n",
    "print(\"Accuracy:\"+str((nltk.classify.accuracy(classifier, testing_set))*100)+\"\\n\")\n",
    "\n",
    "labels = {'negative', 'positive'}\n",
    "\n",
    "from collections import Counter\n",
    "TP, FN, FP = Counter(), Counter(), Counter()\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            TP[i] += int(CM[i,j])\n",
    "        else:\n",
    "            FN[i] += int(CM[i,j])\n",
    "            FP[j] += int(CM[i,j])\n",
    "\n",
    "print(\"label     \\tprecision          \\trecall    \\tf_measure\")\n",
    "for label in sorted(labels):\n",
    "    precision, recall = 0, 0\n",
    "    if TP[label] == 0:\n",
    "        f_measure = 0\n",
    "    else:\n",
    "        precision = float(TP[label]) / (TP[label]+FP[label])\n",
    "        recall = float(TP[label]) / (TP[label]+FN[label])\n",
    "        f_measure = float(2) * (precision * recall) / (precision + recall)\n",
    "    print(label+\"\\t\"+str(precision)+\"\\t\"+str(recall)+\"\\t\"+str(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.99      0.99      0.99      1500\n",
      "   positive       0.99      0.99      0.99      1500\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3000\n",
      "\n",
      "\n",
      "Confussion matrix:\n",
      " [[1490   10]\n",
      " [  14 1486]]\n"
     ]
    }
   ],
   "source": [
    "#sklearn library\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "print ('\\nClasification:\\n', classification_report(gold_result, test_result))\n",
    "print ('\\nConfussion matrix:\\n',confusion_matrix(gold_result, test_result)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "#accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "#print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"I love you mother :)\"\n",
    "data2 =\"I hate my mother   \"\n",
    "\n",
    "words = word_tokenize(data)\n",
    "words = create_word_features(words)\n",
    "classifier.classify(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      :( = True           negati : positi =   2071.0 : 1.0\n",
      "                      :) = True           positi : negati =   1005.4 : 1.0\n",
      "                     See = True           positi : negati =     36.3 : 1.0\n",
      "                 arrived = True           positi : negati =     32.3 : 1.0\n",
      "                    THAT = True           negati : positi =     27.7 : 1.0\n",
      "                    miss = True           negati : positi =     26.5 : 1.0\n",
      "                   Thank = True           positi : negati =     25.3 : 1.0\n",
      "                     x15 = True           negati : positi =     23.7 : 1.0\n",
      "                     sad = True           negati : positi =     22.4 : 1.0\n",
      "                  Thanks = True           positi : negati =     20.6 : 1.0\n",
      "                     Let = True           positi : negati =     19.7 : 1.0\n",
      "                    Love = True           positi : negati =     19.6 : 1.0\n",
      "              bestfriend = True           positi : negati =     18.6 : 1.0\n",
      "                    sick = True           negati : positi =     17.0 : 1.0\n",
      "                    MUCH = True           negati : positi =     15.8 : 1.0\n",
      "               community = True           positi : negati =     15.7 : 1.0\n",
      "               followers = True           positi : negati =     15.0 : 1.0\n",
      "                    glad = True           positi : negati =     15.0 : 1.0\n",
      "                   sorry = True           negati : positi =     13.9 : 1.0\n",
      "                   loves = True           positi : negati =     13.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20) # pop√ºler kelimeler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good bye Party era :(\n",
      "Good bye Party era (\n",
      "Good bye Party era \n"
     ]
    }
   ],
   "source": [
    "#if we are going to remove some characters from our data\n",
    "print(string)\n",
    "print(string.replace(\":\", \"\"))\n",
    "print(string.replace(\":\", \"\").replace(\")\", \"\").replace(\"(\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 3000\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()  \n",
    "\n",
    "neg_reviews = []\n",
    "for fileid in twitter_samples.strings('negative_tweets.json'):\n",
    "    fileid2 = fileid.replace(\":\", \"\").replace(\")\", \"\").replace(\"(\", \"\")\n",
    "    words = tknzr.tokenize(fileid2)\n",
    "   \n",
    "    neg_reviews.append((create_word_features(words), \"negative\"))\n",
    "    \n",
    "#print(neg_reviews[0])    \n",
    "#print(len(neg_reviews))\n",
    "\n",
    "\n",
    "pos_reviews = []\n",
    "\n",
    "for fileid in twitter_samples.strings('positive_tweets.json'):\n",
    "    fileid2 = fileid.replace(\":\", \"\").replace(\")\", \"\").replace(\"(\", \"\")\n",
    "    words = tknzr.tokenize(fileid2)\n",
    "    pos_reviews.append((create_word_features(words), \"positive\"))\n",
    "    \n",
    "#print(pos_reviews[0])    \n",
    "#print(len(pos_reviews))\n",
    "\n",
    "train_set = neg_reviews[:3500] + pos_reviews[:3500]\n",
    "test_set =  neg_reviews[3500:] + pos_reviews[3500:]\n",
    "print(len(train_set),  len(test_set))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.70      0.74      1500\n",
      "   positive       0.73      0.82      0.77      1500\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3000\n",
      "\n",
      "\n",
      "Confussion matrix:\n",
      " [[1043  457]\n",
      " [ 272 1228]]\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "testing_set = test_set\n",
    "test_result = []\n",
    "gold_result = []\n",
    "\n",
    "for i in range(len(testing_set)):\n",
    "    test_result.append(classifier.classify(testing_set[i][0]))\n",
    "    gold_result.append(testing_set[i][1])\n",
    "\n",
    "\n",
    "print ('\\nClasification:\\n', classification_report(gold_result, test_result))\n",
    "print ('\\nConfussion matrix:\\n',confusion_matrix(gold_result, test_result)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
